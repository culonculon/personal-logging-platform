#!/usr/bin/env python3
"""
Personal Logging Platform - Browser Collector (Enhanced)
ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹¬í™” ë¶„ì„í•˜ëŠ” ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import sys
import os
from datetime import datetime
import json

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from collectors.browser_collector import BrowserCollector
from analyzers.search_analyzer import SearchAnalyzer
from analyzers.category_analyzer import CategoryAnalyzer


def save_to_json(data: dict, filename: str):
    """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    output_dir = os.path.join(os.path.dirname(__file__), "..", "output")
    os.makedirs(output_dir, exist_ok=True)
    
    filepath = os.path.join(output_dir, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ë¨: {filepath}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸŒ Personal Logging Platform - Browser Collector (Enhanced)")
    print("=" * 60)
    
    # ìˆ˜ì§‘ê¸° ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
    browser_collector = BrowserCollector()
    search_analyzer = SearchAnalyzer()
    category_analyzer = CategoryAnalyzer()
    
    available_browsers = browser_collector.get_available_browsers()
    if not available_browsers:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë¸Œë¼ìš°ì €ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("Chrome ë˜ëŠ” Safariê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    print(f"ğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ ë¸Œë¼ìš°ì €: {', '.join(available_browsers)}")
    
    try:
        # ì˜¤ëŠ˜ ë‚ ì§œ
        today = datetime.now()
        print(f"ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {today.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # === 1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ===
        print(f"\n" + "=" * 30)
        print("ğŸ“¥ 1ë‹¨ê³„: ë¸Œë¼ìš°ì € ë°ì´í„° ìˆ˜ì§‘")
        print("=" * 30)
        
        # ëª¨ë“  ë¸Œë¼ìš°ì €ì—ì„œ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘
        print(f"ğŸ” ë¸Œë¼ìš°ì € íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ ì¤‘...")
        all_history = browser_collector.collect_all_history()
        
        # íˆìŠ¤í† ë¦¬ ë³‘í•©
        print(f"ğŸ”„ íˆìŠ¤í† ë¦¬ ë³‘í•© ì¤‘...")
        merged_history = browser_collector.merge_histories(all_history)
        print(f"âœ… ì´ {len(merged_history)}ê°œì˜ í†µí•© ê¸°ë¡ ìƒì„±")
        
        if not merged_history:
            print("ğŸ“­ ì˜¤ëŠ˜ ë¸Œë¼ìš°ì§• ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # === 2ë‹¨ê³„: ê²€ìƒ‰ì–´ ë¶„ì„ ===
        print(f"\n" + "=" * 30)
        print("ğŸ” 2ë‹¨ê³„: ê²€ìƒ‰ì–´ ë¶„ì„")
        print("=" * 30)
        
        # ê²€ìƒ‰ì–´ ì¶”ì¶œ ë° ë¶„ì„
        search_queries = browser_collector.extract_all_search_queries(all_history)
        print(f"âœ… {len(search_queries)}ê°œì˜ ê²€ìƒ‰ì–´ ì¶”ì¶œ")
        
        if search_queries:
            search_analysis = search_analyzer.analyze_search_patterns(search_queries)
            search_insights = search_analyzer.get_search_insights(search_queries)
            
            print(f"\nğŸ” ê²€ìƒ‰ ë¶„ì„ ê²°ê³¼:")
            print(f"  â€¢ ì´ ê²€ìƒ‰: {search_analysis.get('total_searches', 0)}íšŒ")
            print(f"  â€¢ ê³ ìœ  ê²€ìƒ‰ì–´: {search_analysis.get('unique_queries', 0)}ê°œ")
            print(f"  â€¢ í‰ê·  ê²€ìƒ‰ì–´ ê¸¸ì´: {search_analysis.get('average_query_length', 0)}ì")
            
            engine_dist = search_analysis.get('engine_distribution', {})
            if engine_dist:
                main_engine = max(engine_dist.items(), key=lambda x: x[1])
                print(f"  â€¢ ì£¼ìš” ê²€ìƒ‰ ì—”ì§„: {main_engine[0]} ({main_engine[1]}íšŒ)")
            
            print(f"\nğŸ’¡ ê²€ìƒ‰ ì¸ì‚¬ì´íŠ¸:")
            for insight in search_insights[:3]:
                print(f"  â€¢ {insight}")
        else:
            search_analysis = {}
            search_insights = []
            print("ğŸ“­ ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # === 3ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ì„ ===
        print(f"\n" + "=" * 30)
        print("ğŸ“‚ 3ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ì„")
        print("=" * 30)
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° ë¶„ì„
        categories = category_analyzer.categorize_websites(merged_history)
        category_analysis = category_analyzer.analyze_category_patterns(categories)
        category_insights = category_analyzer.get_category_insights(categories)
        
        print(f"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì™„ë£Œ")
        print(f"  â€¢ í™œì„± ì¹´í…Œê³ ë¦¬: {category_analysis.get('active_categories', 0)}ê°œ")
        
        top_categories = category_analysis.get('top_categories', [])
        if top_categories:
            print(f"  â€¢ ìƒìœ„ ì¹´í…Œê³ ë¦¬:")
            for i, (category, count) in enumerate(top_categories[:3], 1):
                stats = category_analysis['category_stats'][category]
                print(f"    {i}. {category}: {count}íšŒ ({stats['percentage']}%)")
        
        print(f"\nğŸ’¡ ì¹´í…Œê³ ë¦¬ ì¸ì‚¬ì´íŠ¸:")
        for insight in category_insights[:3]:
            print(f"  â€¢ {insight}")
        
        # === 4ë‹¨ê³„: í†µí•© í†µê³„ ===
        print(f"\n" + "=" * 30)
        print("ğŸ“Š 4ë‹¨ê³„: í†µí•© í†µê³„ ë¶„ì„")
        print("=" * 30)
        
        # í¬ê´„ì ì¸ í†µê³„ ìƒì„±
        comprehensive_stats = browser_collector.get_comprehensive_stats(all_history, merged_history)
        
        print(f"ğŸ“ˆ ì „ì²´ í™œë™ ìš”ì•½:")
        print(f"  â€¢ ì´ ë°©ë¬¸: {comprehensive_stats.get('total_visits', 0)}íšŒ")
        print(f"  â€¢ ê³ ìœ  ë„ë©”ì¸: {comprehensive_stats.get('unique_domains', 0)}ê°œ")
        print(f"  â€¢ í™œì„± ë¸Œë¼ìš°ì €: {len(comprehensive_stats.get('available_browsers', []))}ê°œ")
        
        # ë¸Œë¼ìš°ì €ë³„ í†µê³„
        browser_stats = comprehensive_stats.get('browser_stats', {})
        if browser_stats:
            print(f"\nğŸŒ ë¸Œë¼ìš°ì €ë³„ í™œë™:")
            for browser, stats in browser_stats.items():
                print(f"  â€¢ {browser.title()}: {stats['visits']}íšŒ, {stats['unique_domains']}ê°œ ë„ë©”ì¸")
        
        # ìƒìœ„ ë„ë©”ì¸
        top_domains = comprehensive_stats.get('top_domains', [])
        if top_domains:
            print(f"\nğŸ† ìƒìœ„ ë°©ë¬¸ ë„ë©”ì¸:")
            for i, (domain, count) in enumerate(top_domains[:5], 1):
                print(f"  {i}. {domain}: {count}íšŒ")
        
        # ì‹œê°„ëŒ€ë³„ í™œë™
        hourly_dist = comprehensive_stats.get('hourly_distribution', {})
        if hourly_dist:
            print(f"\nâ° ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´:")
            sorted_hours = sorted(hourly_dist.items())
            max_count = max(hourly_dist.values()) if hourly_dist.values() else 1
            
            # í”¼í¬ ì‹œê°„ ì°¾ê¸°
            peak_hour = max(hourly_dist.items(), key=lambda x: x[1])
            print(f"  â€¢ í”¼í¬ ì‹œê°„: {peak_hour[0]}ì‹œ ({peak_hour[1]}íšŒ)")
            
            # í™œë™ ë¶„í¬ (ê°„ë‹¨í•œ ë§‰ëŒ€ê·¸ë˜í”„)
            print(f"  â€¢ ì‹œê°„ëŒ€ë³„ ë¶„í¬:")
            for hour, count in sorted_hours:
                if count > 0:
                    bar = "â–ˆ" * (count * 15 // max_count)
                    print(f"    {hour:02d}ì‹œ: {count:3d}íšŒ {bar}")
        
        # === 5ë‹¨ê³„: ë°ì´í„° ì €ì¥ ===
        print(f"\n" + "=" * 30)
        print("ğŸ’¾ 5ë‹¨ê³„: ë°ì´í„° ì €ì¥")
        print("=" * 30)
        
        date_str = today.strftime('%Y%m%d_%H%M%S')
        
        # ì™„ì „í•œ ë°ì´í„° ì„¸íŠ¸ ì €ì¥
        complete_data = {
            'metadata': {
                'collection_date': today.strftime('%Y-%m-%d'),
                'collection_timestamp': today.isoformat(),
                'available_browsers': available_browsers,
                'total_records': len(merged_history),
                'version': '1.0'
            },
            'raw_data': {
                'history_by_browser': all_history,
                'merged_history': merged_history
            },
            'search_analysis': {
                'queries': search_queries,
                'analysis': search_analysis,
                'insights': search_insights
            },
            'category_analysis': {
                'categories': categories,
                'analysis': category_analysis,
                'insights': category_insights
            },
            'comprehensive_stats': comprehensive_stats
        }
        
        save_to_json(complete_data, f"browser_complete_{date_str}.json")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_report = {
            'date': today.strftime('%Y-%m-%d'),
            'summary': {
                'total_visits': comprehensive_stats.get('total_visits', 0),
                'unique_domains': comprehensive_stats.get('unique_domains', 0),
                'browsers_used': list(browser_stats.keys()) if browser_stats else [],
                'search_count': len(search_queries),
                'category_count': category_analysis.get('active_categories', 0)
            },
            'highlights': {
                'top_domains': top_domains[:5] if top_domains else [],
                'top_searches': [q['query'] for q in search_queries[:5]] if search_queries else [],
                'top_categories': [(cat, count) for cat, count in top_categories[:5]] if top_categories else [],
                'peak_hour': peak_hour[0] if hourly_dist else None
            },
            'insights': {
                'search': search_insights[:3] if search_insights else [],
                'category': category_insights[:3] if category_insights else [],
                'general': [
                    f"ì´ {comprehensive_stats.get('total_visits', 0)}íšŒ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸",
                    f"{comprehensive_stats.get('unique_domains', 0)}ê°œì˜ ê³ ìœ  ë„ë©”ì¸ ì ‘ì†",
                    f"{len(search_queries)}ê°œì˜ ê²€ìƒ‰ì–´ ì‚¬ìš©" if search_queries else "ê²€ìƒ‰ í™œë™ ì—†ìŒ"
                ]
            }
        }
        
        save_to_json(summary_report, f"browser_summary_{today.strftime('%Y%m%d')}.json")
        
        # ì¹´í…Œê³ ë¦¬ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        category_report_text = category_analyzer.generate_category_report(categories)
        report_filepath = os.path.join(os.path.dirname(__file__), "..", "output", f"category_report_{today.strftime('%Y%m%d')}.txt")
        with open(report_filepath, 'w', encoding='utf-8') as f:
            f.write(category_report_text)
        print(f"ğŸ“„ ì¹´í…Œê³ ë¦¬ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_filepath}")
        
        # === ìµœì¢… ìš”ì•½ ===
        print(f"\n" + "=" * 60)
        print("ğŸ‰ ë¸Œë¼ìš°ì € ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ!")
        print("=" * 60)
        
        print(f"ğŸ“Š ìµœì¢… ìš”ì•½:")
        print(f"  â€¢ ìˆ˜ì§‘ëœ ê¸°ë¡: {len(merged_history)}ê°œ")
        print(f"  â€¢ ë¸Œë¼ìš°ì €: {', '.join(available_browsers)}")
        print(f"  â€¢ ê²€ìƒ‰ì–´: {len(search_queries)}ê°œ")
        print(f"  â€¢ ì¹´í…Œê³ ë¦¬: {category_analysis.get('active_categories', 0)}ê°œ")
        print(f"  â€¢ ì €ì¥ëœ íŒŒì¼: 3ê°œ (ì™„ì „ ë°ì´í„°, ìš”ì•½, ë¦¬í¬íŠ¸)")
        
        if top_categories:
            main_category = top_categories[0]
            print(f"  â€¢ ì£¼ìš” í™œë™: {main_category[0]} ({main_category[1]}íšŒ)")
        
        if hourly_dist:
            print(f"  â€¢ í”¼í¬ ì‹œê°„: {peak_hour[0]}ì‹œ")
        
        print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print(f"  1. output/ í´ë”ì˜ JSON íŒŒì¼ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”")
        print(f"  2. ì•± ì¶”ì ê¸° ê°œë°œì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        print(f"  3. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ Obsidian Daily Notesë¡œ ë³€í™˜í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤")
        
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        print(f"\nğŸ”§ ë¬¸ì œ í•´ê²°:")
        print(f"  â€¢ Chrome/Safari ë¸Œë¼ìš°ì €ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ì¢…ë£Œí•´ë³´ì„¸ìš”")
        print(f"  â€¢ ë””ìŠ¤í¬ ì ‘ê·¼ ê¶Œí•œì„ í™•ì¸í•´ë³´ì„¸ìš”")
        print(f"  â€¢ í„°ë¯¸ë„ì—ì„œ 'python3 main.py' ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”")


if __name__ == "__main__":
    main()
